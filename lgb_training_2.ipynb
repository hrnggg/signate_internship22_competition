{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18567,
     "status": "ok",
     "timestamp": 1612449321773,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "m0GkraBvAbRL",
    "outputId": "94b1f989-0224-40c1-b2e7-9b95790bfe0a"
   },
   "outputs": [],
   "source": [
    "!pip install -q category_encoders xfeat texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26451,
     "status": "ok",
     "timestamp": 1612449329831,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "ZdNKyig9Ah7F",
    "outputId": "24df177a-8d1a-4820-c047-249a8c1a3309"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import minimize\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import string\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import category_encoders as ce\n",
    "import xfeat\n",
    "import texthero as hero\n",
    "from lightgbm import LGBMModel, LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26299,
     "status": "ok",
     "timestamp": 1612449329833,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "Zi2Xur1EAxrY"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.daterministic = True\n",
    "\n",
    "SEED = 42\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25551,
     "status": "ok",
     "timestamp": 1612449336205,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "G1F4ojk4AyuB"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "test = pd.read_csv(DATA_PATH + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1612449352989,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "rEhpUTIMA3nn"
   },
   "outputs": [],
   "source": [
    "TRAIN_LEN = len(train)\n",
    "FOLDS = 5\n",
    "NAME = \"baseline001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 584,
     "status": "ok",
     "timestamp": 1612449353359,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "OLWfZjFnA6Jr"
   },
   "outputs": [],
   "source": [
    "def goal2feature(input_df):\n",
    "    tmp = input_df[\"goal\"]\n",
    "    tmp = tmp.replace(\"100000+\", \"100000-100000\")\n",
    "    tmp = np.array([g.split(\"-\") for g in tmp], dtype=\"int\")\n",
    "    output_df = pd.DataFrame(tmp, columns=[\"goal_min\", \"goal_max\"])\n",
    "    output_df[\"goal_upper_flag\"] = output_df[\"goal_min\"] == 100000\n",
    "    output_df[\"goal_lower_flag\"] = output_df[\"goal_min\"] == 1\n",
    "    output_df[\"goal_mean\"] = output_df[[\"goal_min\", \"goal_max\"]].mean(axis=1)\n",
    "    output_df[\"goal_q25\"] = output_df[[\"goal_min\", \"goal_max\"]].quantile(q=0.25, axis=1)\n",
    "    output_df[\"goal_q75\"] = output_df[[\"goal_min\", \"goal_max\"]].quantile(q=0.75, axis=1)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_numerical_feature(input_df):\n",
    "    cols = [\"duration\"]\n",
    "    return input_df[cols].copy()\n",
    "\n",
    "\n",
    "def get_bins(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df[[\"duration\"]],\n",
    "        goal2feature(input_df),\n",
    "    ], axis=1)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"bins_duration\"] = pd.cut(_input_df[\"duration\"],\n",
    "                                        bins=[-1, 30, 45, 60, 100],\n",
    "                                        labels=['bins_d1', 'bins_d2', 'bins_d3', 'bins_d4'])\n",
    "    output_df[\"bins_goal\"] = pd.cut(_input_df[\"goal_max\"],\n",
    "                                    bins=[-1, 19999, 49999, 79999, 99999, np.inf],\n",
    "                                    labels=['bins_g1', 'bins_g2', 'bins_g3', 'bins_g4', 'bins_g5'])    \n",
    "    return output_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1612449353717,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "sn7JUkYuA_py"
   },
   "outputs": [],
   "source": [
    "def get_cross_cat_features(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        get_bins(input_df)\n",
    "    ], axis=1).astype(str)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"category3\"] = _input_df[\"category1\"] + _input_df[\"category2\"] \n",
    "    output_df[\"country+category1\"] = _input_df[\"country\"] + _input_df[\"category1\"]\n",
    "    output_df[\"country+category2\"] = _input_df[\"country\"] + _input_df[\"category2\"]\n",
    "    output_df[\"country+category3\"] = _input_df[\"country\"] + output_df[\"category3\"]\n",
    "    output_df[\"bins_DurationGoal\"] = _input_df[\"bins_duration\"] + _input_df[\"bins_goal\"]\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_cross_num_features(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df), \n",
    "    ], axis=1)\n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[\"ratio_goalMax_duration\"] = _input_df[\"goal_max\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"ratio_goalMin_duration\"] = _input_df[\"goal_min\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"ratio_goalMean_duration\"] = _input_df[\"goal_mean\"] / (_input_df[\"duration\"] + 1)\n",
    "    output_df[\"prod_goalMax_duration\"] = _input_df[\"goal_max\"] * (_input_df[\"duration\"])\n",
    "    output_df[\"prod_goalMin_duration\"] = _input_df[\"goal_min\"] * (_input_df[\"duration\"])\n",
    "    output_df[\"prod_goalMean_duration\"] = _input_df[\"goal_mean\"] * (_input_df[\"duration\"])    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1612449354413,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "4eK8ApyRBFIS"
   },
   "outputs": [],
   "source": [
    "def get_cbe_features(input_df):\n",
    "    train_target = input_df.iloc[:TRAIN_LEN, :].state\n",
    "    _input_df = pd.concat([\n",
    "        input_df, \n",
    "        get_cross_cat_features(input_df),\n",
    "        get_bins(input_df)\n",
    "    ], axis=1)\n",
    "\n",
    "    cols = [\n",
    "        \"category1\",\n",
    "        \"category2\",\n",
    "        \"category3\",\n",
    "        \"country\",\n",
    "        \"country+category1\",\n",
    "        \"country+category2\",\n",
    "        \"country+category3\",\n",
    "        \"bins_duration\",\n",
    "        \"bins_goal\",\n",
    "        \"bins_DurationGoal\",\n",
    "    ]\n",
    "\n",
    "    encoder = ce.CatBoostEncoder()\n",
    "    output_df = encoder.fit_transform(_input_df.loc[:TRAIN_LEN-1, cols], train_target).add_prefix(\"CBE_\")\n",
    "    if len(input_df) > TRAIN_LEN:\n",
    "        output_df2 = encoder.transform(_input_df.loc[TRAIN_LEN:, cols]).add_prefix(\"CBE_\")\n",
    "        output_df = pd.concat([output_df, output_df2])\n",
    "    return output_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1612449354810,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "7C4m6sVQBQHQ"
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = [\n",
    "        \"goal_min\",\n",
    "        \"goal_max\",\n",
    "        \"goal_mean\",\n",
    "        \"duration\",\n",
    "        \"ratio_goalMax_duration\",\n",
    "        \"ratio_goalMin_duration\",\n",
    "        \"prod_goalMax_duration\",\n",
    "        \"prod_goalMin_duration\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1612449356961,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "f3dw5JnaBV-B"
   },
   "outputs": [],
   "source": [
    "def agg_country(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"country\" \n",
    "    group_values = NUM_FEATURES \n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"] \n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_category1(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category1\"\n",
    "    group_values = NUM_FEATURES\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_category2(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category2\"\n",
    "    group_values = NUM_FEATURES\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "\n",
    "def agg_category3(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"category3\"\n",
    "    group_values = NUM_FEATURES\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()\n",
    "\n",
    "def agg_bins_duration_goal(input_df):\n",
    "    _input_df = pd.concat([\n",
    "        input_df,\n",
    "        goal2feature(input_df),\n",
    "        get_cross_num_features(input_df),\n",
    "        get_cross_cat_features(input_df),\n",
    "    ], axis=1)\n",
    "    group_key = \"bins_DurationGoal\"\n",
    "    group_values = NUM_FEATURES\n",
    "    agg_methods = [\"min\", \"max\", \"mean\", \"std\", \"count\"]\n",
    "    output_df, cols = xfeat.aggregation(_input_df, group_key, group_values, agg_methods)\n",
    "    return output_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1612449358746,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "Kv0D9j-zBcY1"
   },
   "outputs": [],
   "source": [
    "def cleansing_hero_remove_html_tags(input_df, text_col):\n",
    "    custom_pipeline = [\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.remove_html_tags,\n",
    "        hero.preprocessing.remove_urls,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_stopwords,\n",
    "        hero.preprocessing.remove_whitespace,\n",
    "        hero.preprocessing.stem\n",
    "    ]\n",
    "    texts = hero.clean(input_df[text_col], custom_pipeline)\n",
    "    return texts\n",
    "\n",
    "def cleansing_hero_only_text(input_df, text_col):\n",
    "    custom_pipeline = [\n",
    "        hero.preprocessing.fillna,\n",
    "        hero.preprocessing.remove_html_tags,\n",
    "        hero.preprocessing.remove_urls,\n",
    "        hero.preprocessing.lowercase,\n",
    "        hero.preprocessing.remove_digits,\n",
    "        hero.preprocessing.remove_punctuation,\n",
    "        hero.preprocessing.remove_diacritics,\n",
    "        hero.preprocessing.remove_stopwords,\n",
    "        hero.preprocessing.remove_whitespace,\n",
    "        hero.preprocessing.stem\n",
    "    ]\n",
    "    texts = hero.clean(input_df[text_col], custom_pipeline)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def basic_text_features_transformer(input_df, text_columns, cleansing_hero=None, name=\"\"):\n",
    "    def _get_features(dataframe, column):\n",
    "        _df = pd.DataFrame()\n",
    "        _df[column + name + '_num_chars'] = dataframe[column].apply(len)\n",
    "        _df[column + name + '_num_exclamation_marks'] = dataframe[column].apply(lambda x: x.count('!'))\n",
    "        _df[column + name + '_num_question_marks'] = dataframe[column].apply(lambda x: x.count('?'))\n",
    "        _df[column + name + '_num_punctuation'] = dataframe[column].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n",
    "        _df[column + name + '_num_symbols'] = dataframe[column].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n",
    "        _df[column + name + '_num_words'] = dataframe[column].apply(lambda x: len(x.split()))\n",
    "        _df[column + name + '_num_unique_words'] = dataframe[column].apply(lambda x: len(set(w for w in x.split())))\n",
    "        _df[column + name + '_words_vs_unique'] = _df[column + name + '_num_unique_words'] / _df[column + name + '_num_words']\n",
    "        _df[column + name + '_words_unique_diff'] = _df[column + name + '_num_words'] - _df[column + name + '_num_unique_words']\n",
    "        _df[column + name + '_words_vs_chars'] = _df[column + name + '_num_words'] / _df[column + name + '_num_chars']\n",
    "        return _df\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[text_columns] = input_df[text_columns].astype(str).fillna('missing')\n",
    "    for c in text_columns:\n",
    "        if cleansing_hero is not None:\n",
    "            output_df[c] = cleansing_hero(output_df, c)\n",
    "        output_df = _get_features(output_df, c)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def text_vectorizer(input_df, \n",
    "                    text_columns,\n",
    "                    cleansing_hero=None,\n",
    "                    vectorizer=CountVectorizer(),\n",
    "                    transformer=TruncatedSVD(n_components=128),\n",
    "                    name='html_count_svd'):\n",
    "    \n",
    "    output_df = pd.DataFrame()\n",
    "    output_df[text_columns] = input_df[text_columns].astype(str).fillna('missing')\n",
    "    features = []\n",
    "    for c in text_columns:\n",
    "        if cleansing_hero is not None:\n",
    "            output_df[c] = cleansing_hero(output_df, c)\n",
    "\n",
    "        sentence = vectorizer.fit_transform(output_df[c])\n",
    "        feature = transformer.fit_transform(sentence)\n",
    "        num_p = feature.shape[1]\n",
    "        feature = pd.DataFrame(feature, columns=[name+str(num_p) + f'_{i:03}' for i in range(num_p)])\n",
    "        features.append(feature)\n",
    "    output_df = pd.concat(features, axis=1)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_basic_text_features_raw(input_df):\n",
    "    output_df = basic_text_features_transformer(input_df=input_df, text_columns=[\"html_content\"], cleansing_hero=None)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_basic_text_features_removed_html_tags(input_df):\n",
    "    output_df = basic_text_features_transformer(input_df=input_df, \n",
    "                                                text_columns=[\"html_content\"], \n",
    "                                                cleansing_hero=cleansing_hero_remove_html_tags, \n",
    "                                               )\n",
    "    return output_df\n",
    "\n",
    "def get_text_vector_raw__tfidf_sdv64(input_df):\n",
    "    output_df = text_vectorizer(input_df,\n",
    "                                [\"html_content\"],\n",
    "                                cleansing_hero=None,\n",
    "                                vectorizer=TfidfVectorizer(),\n",
    "                                transformer=TruncatedSVD(n_components=64, random_state=2021),\n",
    "                                name=\"raw_html_tfidf_sdv\"\n",
    "                                )\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_text_vector_removed_htnl_tags__tfidf_sdv64(input_df):\n",
    "    output_df = text_vectorizer(input_df,\n",
    "                                [\"html_content\"],\n",
    "                                cleansing_hero=cleansing_hero_remove_html_tags, \n",
    "                                vectorizer=TfidfVectorizer(),\n",
    "                                transformer=TruncatedSVD(n_components=64, random_state=2021),\n",
    "                                name=\"removed_tags_html_tfidf_sdv\"\n",
    "                                )\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def get_text_vector_only_text__tfidf_sdv64(input_df):\n",
    "    output_df = text_vectorizer(input_df,\n",
    "                                [\"html_content\"],\n",
    "                                vectorizer=TfidfVectorizer(),\n",
    "                                cleansing_hero=cleansing_hero_only_text, \n",
    "                                transformer=TruncatedSVD(n_components=64, random_state=2021),\n",
    "                                name=\"only_text_html_tfidf_sdv\"\n",
    "                                )\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 931,
     "status": "ok",
     "timestamp": 1612449360851,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "jfYQCy7eBjia"
   },
   "outputs": [],
   "source": [
    "FUNCS = [\n",
    "        goal2feature,\n",
    "        get_numerical_feature,\n",
    "        get_cbe_features,\n",
    "        get_cross_num_features,\n",
    "        agg_country,\n",
    "        agg_category1,\n",
    "        agg_category2,\n",
    "        agg_category3,\n",
    "        agg_bins_duration_goal,\n",
    "    ]\n",
    "\n",
    "FUNCS_TEXT = [\n",
    "        get_basic_text_features_raw,\n",
    "        get_basic_text_features_removed_html_tags,\n",
    "        get_text_vector_raw__tfidf_sdv64,\n",
    "        get_text_vector_removed_htnl_tags__tfidf_sdv64,\n",
    "        get_text_vector_only_text__tfidf_sdv64,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1612449361164,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "tQ3r9LruBo11"
   },
   "outputs": [],
   "source": [
    "def get_process_funcs():\n",
    "    funcs = FUNCS + FUNCS_TEXT \n",
    "    return funcs\n",
    "\n",
    "def to_feature(input_df, funcs):\n",
    "    output_df = pd.DataFrame()\n",
    "    for func in tqdm(funcs, total=len(funcs)):\n",
    "        _df = func(input_df)\n",
    "        assert len(_df) == len(input_df), func.__name__\n",
    "        output_df = pd.concat([output_df, _df], axis=1)\n",
    "\n",
    "    return output_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 259666,
     "status": "ok",
     "timestamp": 1612449620048,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "_2_Q7NI9BxC5",
    "outputId": "d202eed5-9752-4c25-e939-8e6a597b6d3a"
   },
   "outputs": [],
   "source": [
    "input_df = pd.concat([train, test]).reset_index(drop=True)\n",
    "process_funcs = get_process_funcs()\n",
    "output_df = to_feature(input_df, process_funcs)\n",
    "train_x = output_df.iloc[:len(train)]\n",
    "test_x = output_df.iloc[len(train):].reset_index(drop=True)\n",
    "\n",
    "train_y = train[\"state\"]\n",
    "\n",
    "pickle.dump(train_x, open(\"train_x.pkl\", 'wb'))\n",
    "pickle.dump(test_x, open(\"test_x.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 250892,
     "status": "ok",
     "timestamp": 1612449620049,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "YY2XOdG7CKfX"
   },
   "outputs": [],
   "source": [
    "def make_skf(train_x, train_y, random_state=2021):\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    folds_idx = [(t, v) for (t, v) in skf.split(train_x, train_y)]\n",
    "    return folds_idx\n",
    "\n",
    "\n",
    "def threshold_optimization(y_true, y_pred, metrics=None):\n",
    "    def f1_opt(x):\n",
    "        if metrics is not None:\n",
    "            score = -metrics(y_true, y_pred >= x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return score\n",
    "    result = minimize(f1_opt, x0=np.array([0.5]), method='Nelder-Mead')\n",
    "    best_threshold = result['x'].item()\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def optimized_f1(y_true, y_pred):\n",
    "    bt = threshold_optimization(y_true, y_pred, metrics=f1_score)\n",
    "    score = f1_score(y_true, y_pred >= bt)\n",
    "    return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 249674,
     "status": "ok",
     "timestamp": 1612449620443,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "2X-Ez3i9CR2_"
   },
   "outputs": [],
   "source": [
    "class MyLGBMModel:\n",
    "    def __init__(self, name=None, params=None, fold=None, train_x=None, train_y=None, test_x=None, metrics=None, seeds=None):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.test_x = test_x\n",
    "        self.name = name\n",
    "        self.params = params\n",
    "        self.metrics = metrics\n",
    "        self.kfold = fold\n",
    "        self.oof = None\n",
    "        self.preds = None\n",
    "        self.seeds = seeds if seeds is not None else [2020]  \n",
    "        self.models = {}  \n",
    "\n",
    "    def build_model(self):\n",
    "        model = LGBMModel(**self.params)\n",
    "        return model\n",
    "\n",
    "    def predict_cv(self, pretrained=False, model_dir=\"./\"):\n",
    "        oof_seeds = []\n",
    "        scores_seeds = []\n",
    "        for seed in self.seeds:\n",
    "            oof = []\n",
    "            va_idxes = []\n",
    "            scores = []\n",
    "            train_x = self.train_x.values\n",
    "            train_y = self.train_y.values\n",
    "            fold_idx = self.kfold(self.train_x, self.train_y, random_state=seed) \n",
    "\n",
    "            for cv_num, (tr_idx, va_idx) in enumerate(fold_idx):\n",
    "                tr_x, va_x = train_x[tr_idx], train_x[va_idx]\n",
    "                tr_y, va_y = train_y[tr_idx], train_y[va_idx]\n",
    "                va_idxes.append(va_idx)\n",
    "                model = self.build_model()\n",
    "                model_name = f\"{self.name}_SEED{seed}_FOLD{cv_num}_model.pkl\"\n",
    "                model_path = model_dir + model_name\n",
    "    \n",
    "                if pretrained == False:\n",
    "                  model.fit(tr_x, tr_y,\n",
    "                            eval_set=[[va_x, va_y]],\n",
    "                            early_stopping_rounds=100,\n",
    "                            verbose=False)  \n",
    "                  pickle.dump(model, open(model_name, 'wb'))\n",
    "                else:\n",
    "                  model = pickle.load(open(model_path, 'rb'))\n",
    "                \n",
    "                self.models[model_name] = model  \n",
    "                \n",
    "                pred = model.predict(va_x)\n",
    "                oof.append(pred)\n",
    "\n",
    "                score = self.get_score(va_y, pred)\n",
    "                scores.append(score)\n",
    "                print(f\"SEED:{seed}, FOLD:{cv_num} =====> val_score:{score}\")\n",
    "\n",
    "            va_idxes = np.concatenate(va_idxes)\n",
    "            oof = np.concatenate(oof)\n",
    "            order = np.argsort(va_idxes)\n",
    "            oof = oof[order]\n",
    "            oof_seeds.append(oof)\n",
    "            scores_seeds.append(np.mean(scores))\n",
    "            \n",
    "        oof = np.mean(oof_seeds, axis=0)\n",
    "        self.oof = oof\n",
    "        print(f\"model:{self.name} score:{self.get_score(self.train_y, oof)}\\n\")\n",
    "        return oof      \n",
    "\n",
    "    def inference(self, pretrained=False, model_dir=\"./\"):\n",
    "        preds_seeds = []\n",
    "        for seed in self.seeds:\n",
    "            preds = []\n",
    "            test_x = self.test_x.values\n",
    "            for cv_num in range(FOLDS):\n",
    "                print(f\"-INFERENCE- SEED:{seed}, FOLD:{cv_num}\")\n",
    "\n",
    "                model_name = f\"{self.name}_SEED{seed}_FOLD{cv_num}_model.pkl\"\n",
    "                model_path = model_dir + model_name\n",
    "                if pretrained == False:\n",
    "                  model = self.models[model_name]                \n",
    "                else:\n",
    "                  model = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "                pred = model.predict(test_x)\n",
    "                preds.append(pred)\n",
    "            preds = np.mean(preds, axis=0)\n",
    "            preds_seeds.append(preds)\n",
    "        preds = np.mean(preds_seeds, axis=0)\n",
    "        self.preds = preds\n",
    "        return preds\n",
    "\n",
    "    def tree_importance(self):\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        for i, (tr_idx, va_idx) in enumerate(self.kfold(self.train_x, self.train_y)):\n",
    "            tr_x, va_x = self.train_x.values[tr_idx], self.train_x.values[va_idx]\n",
    "            tr_y, va_y = self.train_y.values[tr_idx], self.train_y.values[va_idx]\n",
    "            model = self.build_model()\n",
    "            model.fit(tr_x, tr_y,\n",
    "                      eval_set=[[va_x, va_y]],\n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose=False) \n",
    "            _df = pd.DataFrame()\n",
    "            _df['feature_importance'] = model.feature_importances_\n",
    "            _df['column'] = self.train_x.columns\n",
    "            _df['fold'] = i + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n",
    "        order = feature_importance_df.groupby('column') \\\n",
    "                    .sum()[['feature_importance']] \\\n",
    "                    .sort_values('feature_importance', ascending=False).index[:50]\n",
    "        fig, ax = plt.subplots(figsize=(12, max(4, len(order) * .2)))\n",
    "        sns.boxenplot(data=feature_importance_df, y='column', x='feature_importance', order=order, ax=ax,\n",
    "                      palette='viridis')\n",
    "        fig.tight_layout()\n",
    "        ax.grid()\n",
    "        ax.set_title('feature importance')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        return fig, feature_importance_df\n",
    "    \n",
    "    def get_score(self, y_true, y_pred):\n",
    "        score = self.metrics(y_true, y_pred)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1487002,
     "status": "ok",
     "timestamp": 1612450971686,
     "user": {
      "displayName": "Hiro NXX",
      "photoUrl": "",
      "userId": "03722192317931833698"
     },
     "user_tz": -540
    },
    "id": "JkGYB5H4CnkE",
    "outputId": "a0ba968f-e299-4dba-dd64-16b8e1764875"
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": 'binary',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 31,\n",
    "    \"random_state\": 2021,\n",
    "    \"n_jobs\": -1,\n",
    "    \"importance_type\": \"gain\",\n",
    "    'colsample_bytree': .5,\n",
    "    \"reg_lambda\": 5,\n",
    "}\n",
    "model = MyLGBMModel(name=NAME, \n",
    "                    params=model_params,\n",
    "                    fold=make_skf,\n",
    "                    train_x=train_x,\n",
    "                    train_y=train_y,\n",
    "                    test_x=test_x,\n",
    "                    metrics=optimized_f1, \n",
    "                    seeds=[0, 1, 2]\n",
    "                   )\n",
    "\n",
    "fig, importance_df = model.tree_importance()\n",
    "\n",
    "selected_num = 300 \n",
    "\n",
    "cols = importance_df.groupby(\"column\").mean().reset_index().sort_values(\"feature_importance\", ascending=False)[\"column\"].tolist()\n",
    "pickle.dump(cols, open(\"cols.pkl\", \"wb\"))\n",
    "selected_cols = cols[:selected_num]\n",
    "\n",
    "model.train_x = model.train_x[selected_cols]\n",
    "model.test_x = model.test_x[selected_cols]\n",
    "\n",
    "oof = model.predict_cv()\n",
    "preds = model.inference()\n",
    "\n",
    "best_threshold = threshold_optimization(y_true=train_y, y_pred=oof, metrics=f1_score) \n",
    "print(f\"best_threshold is {best_threshold}\\n\")\n",
    "\n",
    "labels = preds >= best_threshold"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMts5jKkrcM/L8Ihwnf1MlG",
   "name": "lgb_training_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
